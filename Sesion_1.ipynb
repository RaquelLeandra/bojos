{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPRKSlh97xpY"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_circles, make_classification, make_regression\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "io1qqJsG7xpc"
   },
   "source": [
    "### Funciones Auxiliares\n",
    "\n",
    "Usaremos estas funciones para ver los resultados de nuestros experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UWef931l7xpc"
   },
   "outputs": [],
   "source": [
    "def pintar_resultados_regresion(X,y_real, X_train,y_train_pred,X_test,y_test_pred):\n",
    "    plt.figure()\n",
    "    sns.scatterplot(x=X_train,y=y_train, label = 'Valores de train');\n",
    "    index_train = np.argsort(X_train)\n",
    "    plt.plot(X_train[index_train],y_train_pred[index_train], color='red', label='Predicción de la regresión');\n",
    "    plt.plot(X, y_real, color='green', linewidth=2, label='Función real',alpha=0.2);\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left');\n",
    "    plt.title('Datos de train');\n",
    "\n",
    "    plt.figure()\n",
    "    sns.scatterplot(x=X_test,y=y_test, label = 'Valores de test');\n",
    "    index = np.argsort(X_test)\n",
    "    plt.plot(X_test[index],y_test_pred[index], color='red', label='Predicción de la regresión');\n",
    "    plt.plot(X, y_real, color='green', linewidth=2, label='Función real',alpha=0.2);\n",
    "    plt.title('Datos de test');\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left');\n",
    "\n",
    "\n",
    "\n",
    "def pintar_resultados_clasificacion(X_train, y_train, X_test, y_test):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Valores reales Train')\n",
    "    sns.scatterplot(x=X_train[:,0], y=X_train[:, 1], hue=y_train, marker='o', label='Clases')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Predicciones Train')\n",
    "    sns.scatterplot(x=X_train[:,0], y=X_train[:, 1], hue=y_train_pred, marker='o', label='Clases')\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Valores reales Test')\n",
    "    sns.scatterplot(x=X_test[:,0], y=X_test[:, 1], hue=y_test, marker='x', label='Clases')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Predicciones Test')\n",
    "    sns.scatterplot(x=X_test[:,0], y=X_test[:, 1], hue=y_test_pred, marker='x', label='Clases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5IFpxPh57xpe"
   },
   "source": [
    "# Regresión \n",
    "\n",
    "En una regresión queremos predecir la tendencia que siguen los datos. Siendo estos datos variables continuas.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3fVbVjq7xpf"
   },
   "source": [
    "##  Regresión Lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7rd5J787xpf"
   },
   "source": [
    "Para ver como funciona la regresión lineal vamos a generar datos sintéticos, así tendremos el valor real que seguiría la función que queremos predecir.\n",
    "\n",
    "En concreto, vamos a predecir los valores y de la función:\n",
    "\n",
    "$y = 1 + 0.5x$\n",
    "\n",
    "Para generar los datos sintéticos usaremos la función base y le añadiremos ruido gausiano. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "id": "Yg8p4qoD7xpf",
    "outputId": "1b8c5c00-73da-4249-a25d-87546ebf733e"
   },
   "outputs": [],
   "source": [
    "N = 50\n",
    "noise = 0.4\n",
    "\n",
    "X = np.linspace(0, 10, N)\n",
    "y_real =  1 + 0.5*X \n",
    "\n",
    "y = 1 + 0.5*X + np.random.normal(0,noise,N) #ruido gaussiano\n",
    "\n",
    "sns.scatterplot(x=X,y=y, label='Muestras');\n",
    "plt.plot(X, y_real, color='green', linewidth=2, label='Función Real',alpha=0.2);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBIREagv7xpi"
   },
   "source": [
    "Los separamos en train (2/3) y test (1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "uSOI_MXN7xpi",
    "outputId": "2d1f39c8-c7e1-45a4-e3c8-8d3a1d46d0b1"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "sns.scatterplot(x=X_train, y=y_train, marker='o', label='train')\n",
    "sns.scatterplot(x=X_test, y=y_test,marker='x', label='test')\n",
    "plt.legend()\n",
    "plt.title('Train and test split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "colab_type": "code",
    "id": "14Vxwsz27xpo",
    "outputId": "ab3ba0c3-ac63-4aa5-9edf-3cfcf9646497"
   },
   "outputs": [],
   "source": [
    "# Declaramos el modelo\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Lo entrenamos\n",
    "lr.fit(X_train.reshape(-1, 1), y_train)\n",
    "\n",
    "# Pintamos la predicción sobre los datos\n",
    "y_train_pred = lr.predict(X_train.reshape(-1,1))\n",
    "y_test_pred = lr.predict(X_test.reshape(-1, 1))\n",
    "pintar_resultados_regresion(X,y_real, X_train,y_train_pred,X_test,y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZjuDYvXC18s"
   },
   "source": [
    "\n",
    "Hemos tenido unos muy buenos resultados \n",
    "\n",
    "<img src=https://images-na.ssl-images-amazon.com/images/I/71%2BncdWcmRL.png width=\"150\">\n",
    "\n",
    "\n",
    "Pero... que pasaría si tenemos datos no lineales?\n",
    "\n",
    "Vamos a generar datos sintéticos no lineales para ver que pasa.\n",
    "\n",
    "Esta vez siguiendo la función $y=sin(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "id": "rPbPdlKL7xpr",
    "outputId": "8fb05727-5ca2-4537-a6d4-f77f3ee2e4af"
   },
   "outputs": [],
   "source": [
    "N = 50 # number of data points\n",
    "X = np.linspace(0, 4*np.pi, N)\n",
    "y =np.sin(X)  + np.random.normal(0,0.2,N)\n",
    "y_real = np.sin(X) \n",
    "sns.scatterplot(x=X,y=y, label='Samples')\n",
    "plt.plot(X, y_real, color='green', linewidth=2, label='Function real',alpha=0.2);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "94X83G1w7xpt"
   },
   "source": [
    "Separamos los datos en train (1/3) y test (2/3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "kBwmPPm47xpt",
    "outputId": "381275ae-1ff1-4470-90fc-b1e23bc30da6"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(X_train.shape, X_test.shape)\n",
    "sns.scatterplot(x=X_train, y=y_train, marker='o', label='train')\n",
    "sns.scatterplot(x=X_test, y=y_test,marker='x', label='test')\n",
    "plt.legend();\n",
    "plt.title('Train and test split');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6NXK4_fIYqj"
   },
   "source": [
    "Entrenamos el modelo con los datos nuevos y predecimos el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "ABSDVLUj7xpw",
    "outputId": "5c9f3259-aafd-414e-a4d7-50f6251f5c48"
   },
   "outputs": [],
   "source": [
    "# Declaramos el modelo\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Lo entrenamos\n",
    "lr.fit(X_train.reshape(-1, 1), y_train)\n",
    "\n",
    "# Pintamos la predicción sobre los datos\n",
    "y_train_pred = lr.predict(X_train.reshape(-1, 1))\n",
    "\n",
    "y_test_pred = lr.predict(X_test.reshape(-1, 1))\n",
    "\n",
    "print('Mean squared error: ', mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "pintar_resultados_regresion(X,y_real, X_train,y_train_pred,X_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gvzk6AZMD1RP"
   },
   "source": [
    "Los resultados no son tan buenos.. \n",
    "\n",
    "<img src=https://i1.sndcdn.com/artworks-000475134603-4i982j-t500x500.jpg width=\"150\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9Mz2oQs7xpx"
   },
   "source": [
    "## Regresión polinómica\n",
    "\n",
    "La regresión logística no es suficientemente compleja como para poder predecir los datos de nuestra función. Necesitamos maquinaria más potente. \n",
    "\n",
    "Para solucionar este problema vamos a utilizar una regresión polinómica. O lo que es lo mismo: vamos a aplicar una transformación polinómica a nuestros datos y luego vamos a aplicar una regresión lineal sobre los datos transformados. \n",
    "\n",
    "$Transformación(x) = \\hat{x} = constante + x + x^2 + x^3 + ... x^{n - 1}$\n",
    "\n",
    "$Regresión\\_lineal(\\hat{x}) = Regresión\\_lineal(x + x^2 + x^3 + ... x^{n-1}) = Regresión\\_polinómica(x)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "vNSzq5lU7xpy",
    "outputId": "86fac40e-5461-4c44-c10c-d0bb2410780b"
   },
   "outputs": [],
   "source": [
    "n = 2 # Que pasa si cambiamos el grado máximo de la transformación?\n",
    "poli_transform = PolynomialFeatures(n)\n",
    "X_poli_train = poli_transform.fit_transform(X_train.reshape(-1, 1))\n",
    "X_poli_test = poli_transform.transform(X_test.reshape(-1, 1))\n",
    "\n",
    "model =  LinearRegression()\n",
    "\n",
    "model.fit(X_poli_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_poli_train)\n",
    "\n",
    "y_test_pred = model.predict(X_poli_test)\n",
    "\n",
    "print('Mean squared error: ', mean_squared_error(y_test, y_test_pred))\n",
    "pintar_resultados_regresion(X,y_real, X_train,y_train_pred,X_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZj84qJ8AmQU"
   },
   "source": [
    "Si utilizamos un polinómio de grado 7 aproxíma bastante bien.\n",
    "\n",
    "![feliz.jpg](data:image/jpeg;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PErMHMGl7xpz"
   },
   "source": [
    "# Clasificación\n",
    "\n",
    "Ahora vamos a ver como trabajaríamos con un problema de clasificación. \n",
    "\n",
    "Esta vez nuestros datos van a tener dos dimensiones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G4e5jvhWBZC1"
   },
   "source": [
    "## Regresión Logística\n",
    "\n",
    "A pesar de su nombre, la regresión logística es un modelo de clasificación. \n",
    "\n",
    "Este modelo es muy parecido a la regresión lineal.\n",
    "\n",
    "La unica diferencia es que al resultado de la regresión lineal le aplicamos una función sigmoide. De esta forma, a partir de un resultado continuo obtenemos dos clases. \n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1280px-Logistic-curve.svg.png width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mNmT5qaK7xp0"
   },
   "source": [
    "### Con unos datos fáciles\n",
    "\n",
    "Esta vez vamos a intentar clasificar unos datos linealmente separables. \n",
    "\n",
    "Este tipo de datos no son comunes en la vida real, pero lo vamos a utilizar como ejemplo. \n",
    "\n",
    "Estos datos tienen dos dimensiones, cada punto tendrá un valor para el eje horizontal y otro para el eje vertical.\n",
    "\n",
    "Cada punto corresponderá con una clase, esta clase va s ser lo que queremos predecir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "LbKi99NR7xp0",
    "outputId": "ec27e4ad-2e42-405d-85a5-ff3e32dd506a"
   },
   "outputs": [],
   "source": [
    "N_SAMPLES = 500\n",
    "NOISE = 0\n",
    "N_VARIABLES = 2\n",
    "\n",
    "X, y = make_classification(n_samples=N_SAMPLES, n_features=N_VARIABLES, n_redundant=0, n_informative=N_VARIABLES,\n",
    "                               random_state=1, n_clusters_per_class=1, class_sep=2-NOISE)\n",
    "\n",
    "print(X[:4], y[:4])\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, marker='o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "VXh4OXWk7xp2",
    "outputId": "d23caea5-208f-42d5-bcbf-843ad734b673"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "sns.scatterplot(x=X_train[:,0], y=X_train[:, 1], hue=y_train, marker='o', label='train')\n",
    "sns.scatterplot(x=X_test[:,0], y=X_test[:, 1], hue=y_test,marker='x', label='test')\n",
    "plt.legend();\n",
    "plt.title('Train and test split');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si utilizamos una regresión logística para clasificar este tipo de datos vemos un muy bunen resultado. \n",
    "\n",
    "Nuestro modelo a acertado para todas las muestras!\n",
    "\n",
    "Ahora vamos a ver que pasa si añadimos ruido a los datos. Funciona igual de bien?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "pufyHkkg7xp5",
    "outputId": "c9fb318a-47c9-4c62-f02a-4224d5cd7ba6"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_train_pred = lr.predict(X_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "accuracy_score = lr.score(X_test, y_test)\n",
    "print('Prediction Accuracy: {}'.format( accuracy_score))\n",
    "\n",
    "pintar_resultados_clasificacion(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KfPji3ttBguO"
   },
   "source": [
    "### Con datos un poco más difíciles\n",
    "\n",
    "Vamos a probar que pasa con unos datos un poco más complicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "ZLh_MXVD9Iwu",
    "outputId": "a09aa44d-d96c-46ae-98a9-1d684270dc16"
   },
   "outputs": [],
   "source": [
    "ruido = 0.1\n",
    "\n",
    "X, y= make_moons(n_samples=N_SAMPLES, noise=ruido, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "sns.scatterplot(x=X_train[:,0], y=X_train[:, 1], hue=y_train, marker='o', label='train')\n",
    "sns.scatterplot(x=X_test[:,0], y=X_test[:, 1], hue=y_test,marker='x', label='test')\n",
    "plt.legend();\n",
    "plt.title('Train and test split');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pues no tenemos tan buenos resultados. \n",
    "\n",
    "Esto pasa por que estamos intentando clasificar dos grupos no linealmente separables con un modelo lineal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "HW2IsMiwBJIl",
    "outputId": "1e6c5186-eee7-47c8-8d11-30483a9854fd"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_train_pred = lr.predict(X_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "accuracy_score = lr.score(X_test, y_test)\n",
    "print('Prediction Accuracy: {}'.format( accuracy_score))\n",
    "\n",
    "pintar_resultados_clasificacion(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y con unos datos aun más complicados? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "z0bqjIGRBrUB",
    "outputId": "3e71a21a-f3b1-44a2-c226-10c99172b44d"
   },
   "outputs": [],
   "source": [
    "X, y= make_circles(n_samples=N_SAMPLES, noise=ruido, factor=0.5, random_state=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "sns.scatterplot(x=X_train[:,0], y=X_train[:, 1], hue=y_train, marker='o', label='train')\n",
    "sns.scatterplot(x=X_test[:,0], y=X_test[:, 1], hue=y_test,marker='x', label='test')\n",
    "plt.legend();\n",
    "plt.title('Train and test split');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pues tenemos un puñetero desastre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "GMkOB86rB5GP",
    "outputId": "fa0dbe63-e708-4ac8-851c-3455e1b53156"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_train_pred = lr.predict(X_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "accuracy_score = lr.score(X_test, y_test)\n",
    "print('Prediction Accuracy: {}'.format( accuracy_score))\n",
    "\n",
    "pintar_resultados_clasificacion(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iEj_mL60DQ3M"
   },
   "source": [
    "Para datos no linealmente separables vamos a tener que: \n",
    "* Transformarlos no linealmente, como hicimos en el ejemplo de regresión.\n",
    "* Utilizar un modelo no lineal. \n",
    "\n",
    "Ahora vamos a ver como se comportaría un modelo no lineal. \n",
    "\n",
    "## K-Nearest Neighbors(KNN)\n",
    "\n",
    "Para clasificar una muestra por knn se busca las k muestras más parecidas a esta y se le asigna la clase más común dentro de estas muestras. \n",
    "\n",
    "Para este caso concreto mediremos como de parecidas son dos muestras utilizando la distancia euclidea, pero el concepto de \"parecido\" se puede definir como queramos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "tmkX5gKHFg5G",
    "outputId": "0928f78f-3cb8-42e5-c119-b61994bdc47a"
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=N_SAMPLES, n_features=N_VARIABLES, n_redundant=0, n_informative=N_VARIABLES,\n",
    "                               random_state=1, n_clusters_per_class=1, class_sep=2-NOISE)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1, )\n",
    "knn.fit(X_train, y_train)\n",
    "y_train_pred = knn.predict(X_train)\n",
    "y_test_pred = knn.predict(X_test)\n",
    "accuracy_score = knn.score(X_test, y_test)\n",
    "print('Prediction Accuracy: {}'.format( accuracy_score))\n",
    "\n",
    "pintar_resultados_clasificacion(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "ITqoUfvRDV_l",
    "outputId": "18f2f0b7-e21d-447b-be2e-8b7e641195dc"
   },
   "outputs": [],
   "source": [
    "ruido = 0.1\n",
    "\n",
    "X, y= make_moons(n_samples=N_SAMPLES, noise=ruido, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1, )\n",
    "knn.fit(X_train, y_train)\n",
    "y_train_pred = knn.predict(X_train)\n",
    "y_test_pred = knn.predict(X_test)\n",
    "accuracy_score = knn.score(X_test, y_test)\n",
    "print('Prediction Accuracy: {}'.format( accuracy_score))\n",
    "\n",
    "pintar_resultados_clasificacion(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "NTTS_Zx9FxGr",
    "outputId": "6c3a5990-c090-426c-b1d6-22a071105b39"
   },
   "outputs": [],
   "source": [
    "X, y= make_circles(n_samples=N_SAMPLES, noise=ruido, factor=0.5, random_state=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1, )\n",
    "knn.fit(X_train, y_train)\n",
    "y_train_pred = knn.predict(X_train)\n",
    "y_test_pred = knn.predict(X_test)\n",
    "accuracy_score = knn.score(X_test, y_test)\n",
    "print('Prediction Accuracy: {}'.format( accuracy_score))\n",
    "\n",
    "pintar_resultados_clasificacion(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En estos ejemplos hemos trabajado con datos de juguete. Son datos pequeños de los cuales nosotros conocemos la verdara clase o distribución de las muestras. \n",
    "En el mundo real esto no pasa. Generalmente no vamos a saber la verdad que se oculta de nuestros datos, y generalmente los datos van a ser más complicados. \n",
    "\n",
    "Que pasa si queremos clasificar imágenes? Que pasa si queremos clasificar texto? Estos datos son mucho más complicados y necesitan más potencia de computación. En la sesión siguiente hablaremos de como se puede utilizar un super computador para resolverlos. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pima Diabetes dataset\n",
    "\n",
    "Predecir si un paciente tiene diabetes a partir de sus datos clínicos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as  pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1. Number of times pregnant</th>\n",
       "      <th>2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test</th>\n",
       "      <th>3. Diastolic blood pressure (mm Hg)</th>\n",
       "      <th>4. Triceps skin fold thickness (mm)</th>\n",
       "      <th>5. 2-Hour serum insulin (mu U/ml)</th>\n",
       "      <th>6. Body mass index (weight in kg/(height in m)^2)</th>\n",
       "      <th>7. Diabetes pedigree function</th>\n",
       "      <th>8. Age (years)</th>\n",
       "      <th>9. Class variable (0 or 1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1. Number of times pregnant  \\\n",
       "0                            6   \n",
       "1                            1   \n",
       "2                            8   \n",
       "3                            1   \n",
       "4                            0   \n",
       "\n",
       "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test  \\\n",
       "0                                                148                             \n",
       "1                                                 85                             \n",
       "2                                                183                             \n",
       "3                                                 89                             \n",
       "4                                                137                             \n",
       "\n",
       "   3. Diastolic blood pressure (mm Hg)  4. Triceps skin fold thickness (mm)  \\\n",
       "0                                   72                                   35   \n",
       "1                                   66                                   29   \n",
       "2                                   64                                    0   \n",
       "3                                   66                                   23   \n",
       "4                                   40                                   35   \n",
       "\n",
       "   5. 2-Hour serum insulin (mu U/ml)  \\\n",
       "0                                  0   \n",
       "1                                  0   \n",
       "2                                  0   \n",
       "3                                 94   \n",
       "4                                168   \n",
       "\n",
       "   6. Body mass index (weight in kg/(height in m)^2)  \\\n",
       "0                                               33.6   \n",
       "1                                               26.6   \n",
       "2                                               23.3   \n",
       "3                                               28.1   \n",
       "4                                               43.1   \n",
       "\n",
       "   7. Diabetes pedigree function  8. Age (years)  9. Class variable (0 or 1)  \n",
       "0                          0.627              50                           1  \n",
       "1                          0.351              31                           0  \n",
       "2                          0.672              32                           1  \n",
       "3                          0.167              21                           0  \n",
       "4                          2.288              33                           1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pima = pd.read_csv('https://gist.githubusercontent.com/chaityacshah/899a95deaf8b1930003ae93944fd17d7/raw/3d35de839da708595a444187e9f13237b51a2cbe/pima-indians-diabetes.csv')\n",
    "\n",
    "pima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima_variables = pima.iloc[:,:-1]\n",
    "pima_objetivo = pima.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(pima_variables, pima_objetivo, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_train_pred = lr.predict(X_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "accuracy_score = lr.score(X_test, y_test)\n",
    "print('Prediction Accuracy: {}'.format( accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train, y_train)\n",
    "y_train_pred = knn.predict(X_train)\n",
    "y_test_pred = knn.predict(X_test)\n",
    "accuracy_score = knn.score(X_test, y_test)\n",
    "print('Prediction Accuracy: {}'.format( accuracy_score))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sesion 1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
